{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook processes images and YOLO label files to generate bounding box annotations for Hexbug objects.\n",
    "The pipeline works as follows:\n",
    "1. Load YOLO labels containing subpart bounding boxes (head positions of Hexbugs).\n",
    "2. Use the Segment Anything Model (SAM) to segment the entire frame and detect potential object bounding boxes.\n",
    "3. Match each YOLO bounding box with SAM-detected bounding boxes based on area overlap:\n",
    "   - If a SAM bounding box overlaps with at least 50% of the YOLO bounding box area and is larger, it is selected.\n",
    "   - Refine the selected bounding box by expanding it by 20% to capture the \"whole object.\"\n",
    "4. Save annotations:\n",
    "   - Subpart bounding boxes (class 0) from YOLO labels.\n",
    "   - Whole object bounding boxes (class 1) refined from SAM detections.\n",
    "5. Annotate and save images with bounding boxes for visualization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from segment_anything import sam_model_registry, SamPredictor\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from segment_anything import SamAutomaticMaskGenerator\n",
    "import supervision as sv\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Directories for images and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dir = \"../data/dataset/images/train/\"\n",
    "label_dir = \"../data/dataset/labels/train/\"\n",
    "output_dir = \"../data/dataset/new/\"\n",
    "output_dir_img = output_dir + \"images/\"\n",
    "output_dir_img_bbox = output_dir + \"images_bbox/\"\n",
    "output_dir_label = output_dir + \"labels/\"\n",
    "output_dir_label_seg_only = output_dir + \"labels_seg_only/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(output_dir_img, exist_ok=True)\n",
    "os.makedirs(output_dir_img_bbox, exist_ok=True)\n",
    "os.makedirs(output_dir_label, exist_ok=True)\n",
    "os.makedirs(output_dir_label_seg_only, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load SAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "MODEL_TYPE = \"vit_h\"\n",
    "\n",
    "sam = sam_model_registry[MODEL_TYPE](checkpoint=\"./sam_vit_h_4b8939.pth\")\n",
    "sam.to(device=DEVICE)\n",
    "mask_generator = SamAutomaticMaskGenerator(sam)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read YOLO label files\n",
    "def read_yolo_labels(file_path):\n",
    "    \"\"\"\n",
    "    Reads YOLO label files and parses them into a list of tuples.\n",
    "    Each tuple contains:\n",
    "    - class_id: the class label\n",
    "    - x, y: relative center coordinates\n",
    "    - w, h: relative width and height\n",
    "    \"\"\"\n",
    "    labels = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            class_id, x, y, w, h = map(float, line.strip().split())\n",
    "            labels.append((class_id, x, y, w, h))\n",
    "    return labels\n",
    "\n",
    "# Function to convert relative YOLO coordinates to absolute pixel coordinates\n",
    "def rel_to_abs_center(x, y, w, h, img_w, img_h):\n",
    "    \"\"\"\n",
    "    Converts relative bounding box coordinates to absolute pixel coordinates.\n",
    "    - Input:\n",
    "        x, y: center coordinates (relative)\n",
    "        w, h: width and height (relative)\n",
    "        img_w, img_h: image dimensions in pixels\n",
    "    - Output:\n",
    "        abs_x, abs_y: top-left corner in pixels\n",
    "        abs_w, abs_h: width and height in pixels\n",
    "    \"\"\"\n",
    "    abs_x_center = x * img_w\n",
    "    abs_y_center = y * img_h\n",
    "    abs_w = w * img_w\n",
    "    abs_h = h * img_h\n",
    "    abs_x = abs_x_center - abs_w / 2\n",
    "    abs_y = abs_y_center - abs_h / 2\n",
    "    return abs_x, abs_y, abs_w, abs_h\n",
    "\n",
    "# Function to calculate the intersection area between two bounding boxes\n",
    "def intersection_area(box1, box2):\n",
    "    \"\"\"\n",
    "    Computes the intersection area of two bounding boxes.\n",
    "    - Input:\n",
    "        box1, box2: bounding boxes as (x1, y1, x2, y2)\n",
    "    - Output:\n",
    "        The intersection area (0 if no overlap).\n",
    "    \"\"\"\n",
    "    x1, y1, x2, y2 = box1\n",
    "    x1_, y1_, x2_, y2_ = box2\n",
    "\n",
    "    # Calculate overlap coordinates\n",
    "    xi1 = max(x1, x1_)\n",
    "    yi1 = max(y1, y1_)\n",
    "    xi2 = min(x2, x2_)\n",
    "    yi2 = min(y2, y2_)\n",
    "\n",
    "    # Compute overlap dimensions\n",
    "    inter_width = max(0, xi2 - xi1)\n",
    "    inter_height = max(0, yi2 - yi1)\n",
    "\n",
    "    return inter_width * inter_height\n",
    "\n",
    "# Loop through all images in the directory and process them\n",
    "for img_name in os.listdir(img_dir):\n",
    "    if img_name.endswith('.jpg'):\n",
    "        img_path = os.path.join(img_dir, img_name)\n",
    "        label_path = os.path.join(label_dir, img_name.replace('.jpg', '.txt'))\n",
    "\n",
    "        # Read image and convert to RGB\n",
    "        image_bgr = cv2.imread(img_path)\n",
    "        image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Save original image to output directory\n",
    "        output_img_path = os.path.join(output_dir_img, img_name)\n",
    "        cv2.imwrite(output_img_path, image_bgr)\n",
    "\n",
    "        # Get image dimensions\n",
    "        img_h, img_w, _ = image_rgb.shape\n",
    "\n",
    "        # Generate segmentation masks using SAM\n",
    "        result = mask_generator.generate(image_rgb)\n",
    "        detections = sv.Detections.from_sam(result)\n",
    "\n",
    "        # Read YOLO labels\n",
    "        labels = read_yolo_labels(label_path)\n",
    "\n",
    "        # Annotate the image for visualization\n",
    "        mask_annotator = sv.MaskAnnotator()\n",
    "        annotated_image = image_bgr.copy()\n",
    "\n",
    "        # Lists to store subpart (class 0) and whole object (class 1) labels\n",
    "        subpart_labels = []\n",
    "        whole_object_labels = []\n",
    "\n",
    "        for class_id, rel_x, rel_y, rel_w, rel_h in labels:\n",
    "            # Convert YOLO relative coordinates to absolute coordinates\n",
    "            abs_x, abs_y, abs_w, abs_h = rel_to_abs_center(rel_x, rel_y, rel_w, rel_h, img_w, img_h)\n",
    "            yolo_bbox = (abs_x, abs_y, abs_x + abs_w, abs_y + abs_h)\n",
    "            yolo_area = abs_w * abs_h\n",
    "\n",
    "            # Find the smallest SAM bounding box containing at least 50% of the YOLO bounding box area\n",
    "            best_bbox = None\n",
    "            best_bbox_area = float('inf')\n",
    "\n",
    "            for detection_bbox in detections.xyxy:\n",
    "                detection_bbox_area = (detection_bbox[2] - detection_bbox[0]) * (detection_bbox[3] - detection_bbox[1])\n",
    "                inter_area = intersection_area(yolo_bbox, detection_bbox)\n",
    "                if inter_area >= 0.5 * yolo_area and detection_bbox_area > yolo_area * 2:\n",
    "                    x1, y1, x2, y2 = detection_bbox\n",
    "                    detection_area = (x2 - x1) * (y2 - y1)\n",
    "                    if detection_area < best_bbox_area:\n",
    "                        best_bbox_area = detection_area\n",
    "                        best_bbox = detection_bbox\n",
    "\n",
    "            if best_bbox is not None:\n",
    "                x1, y1, x2, y2 = best_bbox\n",
    "\n",
    "                # Expand bounding box by 20% and ensure it stays within image bounds\n",
    "                new_x1 = max(0, x1 - 0.1 * (x2 - x1))\n",
    "                new_y1 = max(0, y1 - 0.1 * (y2 - y1))\n",
    "                new_x2 = min(img_w, x2 + 0.1 * (x2 - x1))\n",
    "                new_y2 = min(img_h, y2 + 0.1 * (y2 - y1))\n",
    "\n",
    "                # Add to \"whole object\" labels\n",
    "                whole_object_labels.append((1, (new_x1, new_y1, new_x2, new_y2)))\n",
    "\n",
    "                # Draw green bounding box for whole object\n",
    "                cv2.rectangle(annotated_image, (int(new_x1), int(new_y1)), (int(new_x2), int(new_y2)), (0, 255, 0), 2)\n",
    "\n",
    "            # Add YOLO subpart bounding box to labels (class 0)\n",
    "            subpart_labels.append((0, (abs_x, abs_y, abs_x + abs_w, abs_y + abs_h)))\n",
    "\n",
    "            # Draw blue bounding box for subpart\n",
    "            cv2.rectangle(annotated_image, (int(abs_x), int(abs_y)), (int(abs_x + abs_w), int(abs_y + abs_h)), (255, 0, 0), 2)\n",
    "\n",
    "        # Save annotated image\n",
    "        output_img_path = os.path.join(output_dir_img_bbox, img_name)\n",
    "        cv2.imwrite(output_img_path, annotated_image)\n",
    "\n",
    "        # Save labels for subpart (class 0) and whole object (class 1)\n",
    "        with open(os.path.join(output_dir_label, img_name.replace('.jpg', '.txt')), 'w') as file:\n",
    "            for class_id, bbox in subpart_labels + whole_object_labels:\n",
    "                x1, y1, x2, y2 = bbox\n",
    "                rel_x_center = (x1 + (x2 - x1) / 2) / img_w\n",
    "                rel_y_center = (y1 + (y2 - y1) / 2) / img_h\n",
    "                rel_w = (x2 - x1) / img_w\n",
    "                rel_h = (y2 - y1) / img_h\n",
    "                file.write(f\"{class_id} {rel_x_center} {rel_y_center} {rel_w} {rel_h}\\n\")\n",
    "\n",
    "        # Optionally display the annotated image\n",
    "        plt.imshow(cv2.cvtColor(annotated_image, cv2.COLOR_BGR2RGB))\n",
    "        plt.title(f\"Annotated Image - {img_name}\")\n",
    "        plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "segany",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
